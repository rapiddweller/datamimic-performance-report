#!/usr/bin/env python3
"""
CLI for DataMimic performance monitoring using Typer.
Provides two commands:
  - run: Run performance tests for a given script or all scripts.
  - gen-report: Generate a consolidated report from previously collected JSON data.
"""

import json
import os  # Added for environment variable access and file operations.
import yaml  # Added for YAML configuration file parsing.
import typer
from loguru import logger
from app.utils.performance_utils import PerformanceMonitor

app = typer.Typer(help="CLI for DataMimic Performance Monitoring.")


@app.command()
def run(
    config: str = typer.Option(None, help="Path to configuration YAML file."),
    counts: list[int] = typer.Option([100000, 200000, 300000], help="List of record counts."),
    exporters: list[str] = typer.Option(["CSV"], help="List of exporters."),
    num_processes: list[int] = typer.Option([1, 4], help="List of process counts."),
    versions: list[str] = typer.Option(
        ["1.2.1.dev2", "1.2.2.dev23"],
        help="List of datamimic-ce versions to test (if not 'current')."
    ),
    iterations: int = typer.Option(1, help="Number of iterations per test configuration."),
    script: str = typer.Option(
        None, help="Optional specific script filename (e.g., 'example.xml'). If not provided, all scripts are processed."
    )
):
    """
    Run performance tests using the given parameters.
    
    Accepts a YAML configuration file via --config to override default parameters,
    allowing centralized configuration. Additionally, if the environment variable
    DATAMIMIC_VERSIONS is set (as a comma-separated list), it will override the versions
    specified either by the command line or the configuration file.
    
    If --script is provided, it will run tests for that script only;
    otherwise, it iterates over all XML scripts in the scripts directory.
    
    NOTE: This command installs the specified version, executes all scripts and logs performance metrics,
    saving the results to JSON files. No HTML/text reports are generated by this command.
    """
    # Load configuration from file if provided.
    if config:
        try:
            with open(config, "r") as f:
                config_data = yaml.safe_load(f)
            logger.info("Configuration loaded from {}", config)
        except Exception as e:
            logger.error("Error reading configuration file {}: {}", config, e)
            raise typer.Exit(code=1)
        
        # Override command-line parameters with config file values if available.
        counts = config_data.get("counts", counts)
        exporters = config_data.get("exporters", exporters)
        num_processes = config_data.get("num_processes", num_processes)
        iterations = config_data.get("iterations", iterations)
        versions = config_data.get("versions", versions)
    
    # Override versions from environment variable if set.
    env_versions = os.getenv("DATAMIMIC_VERSIONS")
    if env_versions:
        versions = [v.strip() for v in env_versions.split(",") if v.strip()]
        logger.info("Versions overridden by DATAMIMIC_VERSIONS environment variable: {}", versions)
    
    monitor = PerformanceMonitor()
    logger.info(
        "Starting performance monitoring with counts: {}, exporters: {}, num_processes: {}, iterations: {}",
        counts,
        exporters,
        num_processes,
        iterations,
    )

    if script:
        script_file = monitor.scripts_dir / script
        if not script_file.exists():
            logger.error(f"Script {script} not found in {monitor.scripts_dir}")
            raise typer.Exit(code=1)
        logger.info(f"Processing specified script: {script_file}")
        aggregated_results = monitor.collect_performance_data(
            counts=counts,
            exporters=exporters,
            num_processes=num_processes,
            iterations=iterations,
            versions=versions,
            script_file=script_file
        )
    else:
        logger.info("Processing all scripts in the scripts directory")
        aggregated_results = monitor.collect_performance_data(
            counts=counts,
            exporters=exporters,
            num_processes=num_processes,
            iterations=iterations,
            versions=versions
        )

    logger.info("Performance monitoring completed. JSON result files have been generated.")


@app.command()
def gen_report():
    """
    Generate a consolidated report from previously collected JSON data.
    
    This command reads the JSON result files generated by the run command,
    aggregates the performance metrics by version, and creates a consolidated HTML report.
    """
    monitor = PerformanceMonitor()
    results_dir = monitor._exec_dir / "results"
    aggregated_results = {}

    # Iterate over all JSON result files.
    for json_file in results_dir.glob("results_*.json"):
        try:
            with json_file.open("r") as f:
                data = json.load(f)
        except Exception as e:
            logger.error(f"Error reading file {json_file}: {e}")
            continue
        if not data:
            continue
        version = data[0].get("version", "unknown")
        if version not in aggregated_results:
            aggregated_results[version] = []
        aggregated_results[version].extend(data)
        logger.info(f"Loaded {len(data)} results from {json_file}")

    if not aggregated_results:
        logger.error("No result data found in the results directory.")
        raise typer.Exit(code=1)

    logger.info("Generating consolidated report from collected data.")
    monitor.generate_consolidated_report(aggregated_results)
    logger.info("Consolidated report generated successfully.")


if __name__ == "__main__":
    app()
