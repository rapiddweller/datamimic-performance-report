#!/usr/bin/env python3
"""
CLI for DataMimic performance monitoring using Typer.
Provides two commands:
  - run: Run performance tests for a given script or all scripts.
  - gen-report: Generate a consolidated report from previously collected JSON data.
"""

import json
import os  # Added for environment variable access and file operations.

import typer
import yaml  # Added for YAML configuration file parsing.
from loguru import logger

from app.utils.performance_utils import PerformanceMonitor

app = typer.Typer(help="CLI for DataMimic Performance Monitoring.")

# Define constants for default option values
DEFAULT_COUNTS = [100000, 200000, 300000]
DEFAULT_EXPORTERS = ["CSV"]
DEFAULT_NUM_PROCESSES = [1, 4]
DEFAULT_VERSIONS = ["1.2.1.dev2", "1.2.2.dev23"]
DEFAULT_ITERATIONS = 1
DEFAULT_SCRIPT = None
DEFAULT_SCRIPT_HELP = (
    "Optional specific script filename (e.g., 'example.xml'). If not provided, all scripts are processed."
)

# Define the option objects
CONFIG_OPTION = typer.Option(None, help="Path to configuration YAML file.")
COUNTS_OPTION = typer.Option(DEFAULT_COUNTS, help="List of record counts.")
EXPORTERS_OPTION = typer.Option(DEFAULT_EXPORTERS, help="List of exporters.")
NUM_PROCESSES_OPTION = typer.Option(DEFAULT_NUM_PROCESSES, help="List of process counts.")
VERSIONS_OPTION = typer.Option(DEFAULT_VERSIONS, help="List of datamimic-ce versions to test (if not 'current').")
ITERATIONS_OPTION = typer.Option(DEFAULT_ITERATIONS, help="Number of iterations per test configuration.")
SCRIPT_OPTION = typer.Option(DEFAULT_SCRIPT, help=DEFAULT_SCRIPT_HELP)


@app.command()
def run(
    config: str = CONFIG_OPTION,
    counts: list[int] = COUNTS_OPTION,
    exporters: list[str] = EXPORTERS_OPTION,
    num_processes: list[int] = NUM_PROCESSES_OPTION,
    versions: list[str] = VERSIONS_OPTION,
    iterations: int = ITERATIONS_OPTION,
    script: str = SCRIPT_OPTION,
):
    """
    Run performance tests using the given parameters.

    Accepts a YAML configuration file via --config to override default parameters,
    allowing centralized configuration. Additionally, if the environment variable
    DATAMIMIC_VERSIONS is set (as a comma-separated list), it will override the versions
    specified either by the command line or the configuration file.

    If --script is provided, it will run tests for that script only;
    otherwise, it iterates over all XML scripts in the scripts directory.

    NOTE: This command installs the specified version, executes all scripts and logs performance metrics,
    saving the results to JSON files. No HTML/text reports are generated by this command.
    """
    # Load configuration from file if provided.
    if config:
        try:
            with open(config) as f:
                config_data = yaml.safe_load(f)
            logger.info("Configuration loaded from {}", config)
        except Exception as e:
            logger.error("Error reading configuration file {}: {}", config, e)
            raise typer.Exit(code=1) from e

        # Override command-line parameters with config file values if available.
        counts = config_data.get("counts", counts)
        exporters = config_data.get("exporters", exporters)
        num_processes = config_data.get("num_processes", num_processes)
        iterations = config_data.get("iterations", iterations)
        versions = config_data.get("versions", versions)

    # Override versions from environment variable if set.
    env_versions = os.getenv("DATAMIMIC_VERSIONS")
    if env_versions:
        versions = [v.strip() for v in env_versions.split(",") if v.strip()]
        logger.info("Versions overridden by DATAMIMIC_VERSIONS environment variable: {}", versions)

    monitor = PerformanceMonitor()
    logger.info(
        "Starting performance monitoring with counts: {}, exporters: {}, num_processes: {}, iterations: {}",
        counts,
        exporters,
        num_processes,
        iterations,
    )

    if script:
        script_file = monitor.scripts_dir / script
        if not script_file.exists():
            logger.error(f"Script {script} not found in {monitor.scripts_dir}")
            raise typer.Exit(code=1)
        logger.info(f"Processing specified script: {script_file}")
        monitor.collect_performance_data(
            counts=counts,
            exporters=exporters,
            num_processes=num_processes,
            iterations=iterations,
            versions=versions,
            script_file=script_file,
        )
    else:
        logger.info("Processing all scripts in the scripts directory")
        monitor.collect_performance_data(
            counts=counts, exporters=exporters, num_processes=num_processes, iterations=iterations, versions=versions
        )

    logger.info("Performance monitoring completed. JSON result files have been generated.")


@app.command()
def gen_report():
    """
    Generate a consolidated report from previously collected JSON data.

    This command reads the JSON result files generated by the run command,
    aggregates the performance metrics by version, and creates a consolidated HTML report.
    """
    monitor = PerformanceMonitor()
    results_dir = monitor._exec_dir / "results"
    aggregated_results = {}

    # Iterate over all JSON result files.
    for json_file in results_dir.glob("results_*.json"):
        try:
            with json_file.open("r") as f:
                data = json.load(f)
        except Exception as e:
            logger.error(f"Error reading file {json_file}: {e}")
            continue
        if not data:
            continue
        version = data[0].get("version", "unknown")
        if version not in aggregated_results:
            aggregated_results[version] = []
        aggregated_results[version].extend(data)
        logger.info(f"Loaded {len(data)} results from {json_file}")

    if not aggregated_results:
        logger.error("No result data found in the results directory.")
        raise typer.Exit(code=1)

    logger.info("Generating consolidated report from collected data.")
    monitor.generate_consolidated_report(aggregated_results)
    logger.info("Consolidated report generated successfully.")


if __name__ == "__main__":
    app()
